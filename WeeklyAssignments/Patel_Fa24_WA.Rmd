---
title: "D2SC: Weekly Assignments"
output:
  html_document:
    toc: true
    toc_float: true
      collapsed: false
      smooth_scroll: false
author: "Abhi Patel"
date: "`r Sys.Date()`"
output: html_notebook
---
# Initial Loading

```{r}
library(tidyverse)
```
# Week 1

To be able to figure out how to load a library in R, I went to google and searched up "load libraries in r". Then, I clicked on the top result as the description seemed to be what I was looking for. This was https://bookdown.org/nana/intror/install-and-load-packages.html. This gave me the instructions for both installing packages as well as loading libraries. I used the loading libraries instruction it suggested and ran it in a R code chunk. It ended up working correctly.

```{r}
? tidyverse
```
The 'tidyverse' is a set of packages that work in harmony because they share common data representations and 'API' design. This package is designed to make it easy to install and load multiple 'tidyverse' packages in a single step. Learn more about the 'tidyverse' at https://www.tidyverse.org.

# Assignment 2

```{r}
relational_task<-read_csv("tidy_data/MFIndD_analogy.csv")
print(relational_task)
```
"qualtrics_id" is the column that contains the unique identifier for each participant.

```{r}
glimpse(relational_task)
```
The dataset has 792 rows and 6 columns. I got this information using the glimpse() function which allows us to get an idea of the data that we are seeing. This function also shows the number of rows and columns as a part of this

```{r}
relational_task%>%
  distinct(qualtrics_id)%>%
  count()
```
There are 99 people in the dataset.

```{r}
relational_task%>%
  group_by(qualtrics_id)%>%
  summarize(num_trials = n())%>%
  distinct(num_trials)
```
All participants have data from the same number of trials. In this dataset, it seems to be 8 trials per participant.

# Assignment 3
```{r}
library(ggplot2)
```
```{r}
relational_response_type<-
relational_task%>%
  group_by(qualtrics_id)%>%
  summarize(num_relational_matches = sum(response_type=="Rel"))%>%
  ungroup()

```
```{r}
relational_response_type %>%
  ggplot(aes(x=num_relational_matches)) +
  geom_histogram()+
  labs(title = "Number of Relational Responses per Participant", y = "Number of Participants", x = "Number of Relational Matches")
```
I notice that 0 and 8 relational matches seem to have the highest amount of participants associated with . All the number of relational matches in the middle seem to be considerably smaller and relatively even. Perhaps, this may suggest that a person has a tendency to either choose a relational_match or not choose it at all.
```{r}
reshaped_relational_task<-
  relational_task%>%
  select(qualtrics_id, trial_number, response_type) %>% 
  arrange(trial_number)%>%
  pivot_wider(names_from = trial_number, values_from = response_type)
  reshaped_relational_task
  
```
```{r, eval = FALSE}
relational_response_type %>%
  ggplot(aes(x=num_relational_matches, 
             y = qualtrics_id)) +
  geom_histogram()+
  labs(title = "Number of Relational Responses per Participant", 
       y = "Number of Participants", 
       x = "Number of Relational Matches")
```
It turns out that you can only set one axes equal to a column in the aesthetic. I was trying to do it as if setting the x and y variables for a graph, but it wasn't working like that. Nor would it have been correct in hindsight for the question asked. I resolved the error by only having x = num_relational_matches such that we have a histogram that answers the question and solves the error.

# Assignment 4
```{r}
library(ggplot2)
library(dplyr)
```
```{r}
rational_task<-read_csv("tidy_data/MFIndD_REI.csv")
head(rational_task)
typeof(rational_task$scored_response)

```
The column type of the response column is type character. The column type of the scored_response column is type double. They are different even they both contain numbers because the response column includes 1-5 and Strongly Disagree to Strongly Agree as t for one and the other in the response as per the likert scale. The scored response is a double that takes into account any reverse scoring to give a numerical output.

```{r}
rational_task <- rational_task %>%
  mutate(response_numeric = case_when(
    response == "Strongly Disagree" ~ 1,
    response == "Disagree" ~ 2,
    response == "Neutral" ~ 3,
    response == "Agree" ~ 4,
    response == "Strongly Agree" ~ 5,
    response == "1" ~ 1,
    response == "2" ~ 2,
    response == "3" ~ 3,
    response == "4" ~ 4,
    response == "5" ~ 5,
    TRUE ~ NA_real_  # NA for any unexpected values
  ))
rational_task

```
```{r}
rational_task<-rational_task%>%
  mutate(new_scored_response = ifelse(!is.na(rev_scoring),
                                      6-response_numeric,
                                      response_numeric))
rational_task
```
```{r}
differences<-rational_task%>%
  filter(scored_response!=new_scored_response)
differences
```

# Assignment 5
```{r}
summarized_REI<-read_csv("tidy_data/MFIndD_REI.csv")%>%
  group_by(qualtrics_id, sub_type)%>%
  summarize(score = sum(scored_response))
print(summarized_REI)

```
```{r}
NA_scores <- summarized_REI %>%
  filter(is.na(score))
NA_scores
?summarize
```

Yes, there are NAs

```{r}
summarized_REI<-read_csv("tidy_data/MFIndD_REI.csv")%>%
  group_by(qualtrics_id, sub_type)%>%
  summarize(score = sum(scored_response, na.rm = TRUE))
print(summarized_REI)
NA_scores <- summarized_REI %>%
  filter(is.na(score))
NA_scores
```
```{r}
combined_data<-
  left_join(summarized_REI, relational_response_type, by="qualtrics_id")%>%
  filter(!is.na(num_relational_matches))
combined_data
```
```{r}
combined_data %>%
  ggplot(aes(
    x=num_relational_matches, 
    y=score, 
    color=sub_type)) +
  geom_point()+
  geom_smooth(method = "lm")+
  labs(title = "REI Score vs Analogy Score", 
       y = "REI Score", 
       x = "Analogy Score")
  
```
The relation in the scatter plot seems to be that the maximum REI Scores happen at around the Analogy score of 0 and 8. If we are looking at the regression lines, it seems that the subtypes in order of the highest REI Scores to lowest across the analogy scores, seem to be RA, RE,EA,and EE.

# Assignment 6
```{r}
prob_task<-read_csv("tidy_data/MFIndD_probtask.csv")
prob_task
```
```{r}
prob_task%>%
  distinct(condition)
```
There are 4 distinct values in the "condition" column.

```{r}
prob_condition<-prob_task%>%
  distinct(condition)%>%
  pull(condition)
prob_condition
is.vector(prob_condition)
```

```{r}
mean_reaction_time<-numeric(length(prob_condition))
length(mean_reaction_time)
prob_task%>%
  group_by(condition)%>%
  summarize(mean_reaction_time = mean(rt))
for(i in 1:length(mean_reaction_time))
{
  mean_reaction_time[i] = prob_task%>%
                          filter(condition==prob_condition[i])%>%
                          summarize(mean_reaction_time= mean(rt))%>%
                          pull(mean_reaction_time)
}
mean_reaction_time
  
```
```{r}
summary_table_without<-prob_task%>%
  group_by(condition)%>%
  summarize(mean_reaction_time = mean(rt), 
            accuracy = mean(correct))
summary_table_without
```
```{r}
summary_table_across<-prob_task%>%
  group_by(condition)%>%
  summarize(across(
    .cols = c(mean_reaction_time =rt, accuracy=correct), 
    .fns = mean))
summary_table_across
```

# Assignment 7
```{r}
prob_task<-read_csv("tidy_data/MFIndD_probtask.csv")
prob_task
```
```{r}
prob_task %>%
  group_by(condition) %>%
  summarise(across(c(rt, correct), mean)) %>%
  pivot_longer(c(rt, correct)) %>%
  ggplot(aes(y = value, x = condition)) +
  geom_point(color = "red") +
  facet_wrap(~name, scales = "free")
```
1. A. This graph shows that the blob_stacked and the dots_Eq_SizeRand conditions have the higher accuracy as compared to the lower accuracies of blob_shifted and dots_EqSizeSep. For the reaction times, it seems that the lower accuracy tasks resulted in the higher reaction times where as the higher accuracy ones resulted in slightly lower reaction times 

1. B. The first thing I noticed was the accuracy. I was trying to determine any pattern I can see in accuracy. There is no direct pattern I can tell from the graph alone as per accuracy, but given what each task looks like, I can infer that easily "discernible" tasks will result in a higher accuracy. However, it is clear that easily discernible has a different definition for the blobs vs the dots. Then I looked at the reaction time to see if there was any correlation with the accuracy. I was able to determine that there does seem to be. But the underlying reason from the plots alone can't be told.

1.C. The information that is hard to notice from the graphs alone is what the correlation is between the high accuracy tasks vs the low accuracy tasks. You can't necessarily separate it based on blob vs dots as that is clearly not the case here. So there is some other underlying pattern that is resulting in the accuracy differences.

```{r}
summarized_prob_task<-prob_task%>%
  group_by(SubID, condition)%>%
  summarize(prop_corr = mean(correct, na.rm=TRUE))
head(summarized_prob_task)

```
```{r}
library(dplyr)
library(ggplot2)

ggplot(summarized_prob_task, aes(x = condition, y = prop_corr, color = condition)) +
  stat_summary(fun = mean, geom = "point", size = 3) +
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2) +
  labs(title = "Mean Proportion Correct by Condition", y = "Proportion Correct", x = "Condition") +
  theme_minimal() +
  theme(legend.position = "none")
```
Adding error bars to show confidence intervals lets us see how much accuracy varies among participants for each condition. This gives a better sense of consistency within each group.
Next, setting the y-axis from 0 to 1 keeps the scale honest by matching the proportion values, which prevents any visual tricks that might exaggerate small differences.

```{r}
install.packages("ggdist")
library(ggdist)
library(dplyr)
library(ggplot2)

```
```{r}
ggplot(summarized_prob_task, aes(x = condition, y = prop_corr, color = condition, fill = condition)) +
  geom_violin(alpha = 0.3, color = NA) +  # Violin plot for distribution shape per each condition.
  stat_summary(fun = mean, geom = "point", size = 3, color = "black") +  
  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2, color = "black") +  # CI bars
  labs(title = "Mean Proportion Correct by Condition with Violin Distribution",
       y = "Proportion Correct", x = "Condition") +
  theme_minimal() +
  theme(legend.position = "none")
```
The violin plot shows the shape of the distribution revealing any potential clusters in each group. It allows to determine if there is any skewed distributions. I think we are now able to see more distinct shapes/patterns for each different category that we are considering.


